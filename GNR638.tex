\documentclass[12pt,a4paper,twoside]{book}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage{geometry}
% Book-like two-sided margins: larger inner margin for binding
\geometry{a4paper, inner=1in, outer=1cm, top=2cm, bottom=2cm}
\usepackage{amsmath, amssymb, amsfonts} % Core math packages
\usepackage{mathtools} % For advanced math layout
\usepackage{graphicx}  % For inserting images
\usepackage{hyperref}  % For clickable links in ToC
\usepackage{float}     % To control figure placement
\usepackage{parskip}   % Adds space between paragraphs
\usepackage{bm}        % For bold math symbols (vectors/matrices)
\usepackage{tcolorbox} % For colored boxes
\usepackage{marginnote} % For margin notes and side content

% --- Custom Commands for Notation ---
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % Norm command ||x||
\newcommand{\bx}{\mathbf{x}} % Bold x for vectors
\newcommand{\by}{\mathbf{y}} % Bold y for vectors
\newcommand{\Hmat}{\mathbf{H}} % Transformation matrix H

% --- Title Page Info ---
\title{
    \textbf{GNR638: Feature Extraction from Images}\\
    \large Detailed Mathematical Notes \& Derivations
}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle
\frontmatter
\tableofcontents
\cleardoublepage
\mainmatter

% Now you can add chapters with:
% \chapter{Introduction}
% Chapter text...
\newpage
\chapter{Vision Features}
\section{Image Features Vectors}
Image feature vectors are encoding tool that help represent images in a way that is suitable for Machine Learning Tasks and Algorithms.\\
Mathematically, an image (or a patch of an image) is a high-dimensional object. If you have an image patch of size $N \times M$ pixels, raw data lives in $\mathbb{R}^{N \times M}$.  A feature extraction function $f$ maps this raw data into a more manageable, meaningful vector space $\mathbb{R}^d$ (where $d$ is the feature dimension)\\
$$\mathbf{x} = f(\text{Image}) \in \mathbb{R}^d$$The vector $\mathbf{x} = [x_1, x_2, \dots, x_d]^T$ is the feature vector. Each component $x_i$ captures a specific characteristic of the image, such as color intensity, texture patterns, edge orientations, or more complex attributes learned through deep learning models.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Media/i10011.png}
    \caption{Block diagram of the system}
    \label{fig:Feature vector}
\end{figure}

\subsection{Why are they Useful?}
Feature vectors makes it possible for ML to work. Example we want to vectorise the images and then cluster them in the $\mathbb{R^d}$ space, such that within class variance is very low and between class variance is high.\\
\begin{itemize}
    \item vectorisation $\Rightarrow$ numerical representation of images
    \item clustering $\Rightarrow$ grouping similar images together
    \item within class variance $\Rightarrow$ how similar images in the same group are
    \item between class variance $\Rightarrow$ how different images in different groups are
\end{itemize}

\section{Visual Features}
There are 4 types of visual features:
\begin{itemize}
    \item Color Features
    \item Texture Features
    \item Shape Features
    \item Deep features
\end{itemize}

\section{Color Features}
\subsection{Binning}
Image $\rightarrow$ 3 color channels (R,G,B) $\rightarrow$ 0 to 255 (256) values $\rightarrow$ Total Combinations = $256^3$ .\\
Too many combinations, so we reduce them by grouping the near values into Bins.\\
The Math of Binning:For a pixel intensity $p$, its bin index $b$ is calculated as:$$b = \left\lfloor \frac{p}{W} \right\rfloor$$Example: If Pixel Value = 100 and Bin Width = 32.$b = \lfloor 100 / 32 \rfloor = \lfloor 3.125 \rfloor = 3$.So, pixel 100 falls into Bin 3.

\subsection{Color Histogram}
\textbf{Creation of Histogram} is just the number of  pixels falling into a specific bin
Mathematically the equation for histogram bin count is given by -
$$h[k] = \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} \mathbb{I}\left( \left\lfloor \frac{I_c(x,y)}{\text{bin\_width}} \right\rfloor = k \right)$$
The Issue with color histogram is it is tied to the size of the image and therefore two images having the same color profile can have different lokking histograms if there is difference in size of the images to deal with this shastro mein Normalisation ka zikr hai !!\\
\textbf{Normalisation of Histogram} is done by dividing each bin count by the total number of pixels in the image. $\Rightarrow$ histogram = probability distribution making it invariant to image size.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Media/i10012.png}
    \caption{Block diagram of the system}
    \label{fig:Histogram}
\end{figure}

\subsection{Parametric Density Estimation}
\textbf{The Idea} is to instead of storing the entire histogram as color feature we reduce the features to just Mean and Variance of the histogram or a Gaussian curve fitted over the Histogram Data. There are 3 methods in which we can do it.\\
\paragraph{Method 1: Per-Channel Statistics (Independent Gaussians)} This is the simplest approach. We assume the Red, Green, and Blue channels are completely unrelated (independent). We simply ask: "What is the average Red?" and "How much does the Red vary?"The Math: For each channel $c \in \{R, G, B\}$, we compute two statistics over $N$ pixels:Mean ($\mu_c$): The average intensity.$$\mu_{c} = \frac{1}{N}\sum_{i=1}^{N} x_{i,c}$$Standard Deviation ($\sigma_c$): The spread/contrast.$$\sigma_{c} = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (x_{i,c} - \mu_{c})^2}$$The Feature Vector: We store these values for all 3 channels.$$\mathbf{f} = [\mu_R, \sigma_R, \mu_G, \sigma_G, \mu_B, \sigma_B]$$Dimension: 6 parameters2.Limitation: It destroys the relationship between colors. If an image has many Yellow pixels (High Red + High Green), this method just sees "High Red" and "High Green" independently. It doesn't know they occurred together in the same pixel.
\paragraph{Method 2: 3D Multivariate Gaussian (Single Gaussian)} This method treats the color as a single 3D vector $\mathbf{x} = [R, G, B]^T$. It fits a 3D ellipsoid to the data cloud. This captures not just the spread, but the correlation between channels.The Math:Mean Vector ($\boldsymbol{\mu}$): A 3D vector representing the center of the color cloud.$$\boldsymbol{\mu} = \frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_i$$Covariance Matrix ($\Sigma$): A $3 \times 3$ symmetric matrix capturing how channels move together.$$\Sigma = \frac{1}{N}\sum_{i=1}^{N} (\mathbf{x}_i - \boldsymbol{\mu})(\mathbf{x}_i - \boldsymbol{\mu})^T$$Understanding Covariance:The diagonal entries ($\Sigma_{11}, \Sigma_{22}, \Sigma_{33}$) are the variances of R, G, B (same as Method 1).The off-diagonal entries (e.g., $\Sigma_{12}$) tell us if Red and Green are correlated. If $\Sigma_{RG}$ is high, it means the image contains colors like Yellow (R+G) or Cyan, rather than just random noise.The Feature Vector:3 values for $\boldsymbol{\mu}$.6 values for $\Sigma$ (since it is symmetric, $\Sigma_{RG} = \Sigma_{GR}$, so we only need the unique upper-triangular elements)3.Dimension: $3 + 6 = \mathbf{9 \text{ parameters}}$.
\paragraph{Method 3: Gaussian Mixture Models (GMM) } The previous methods assume the image has only one dominant color cluster (unimodal). But what if the image has a red shirt, blue sky, and green grass? A single Gaussian would try to fit a giant blob in the middle (which would be gray), failing to capture the distinct colors.GMM fits $K$ different Gaussians to the data simultaneously.The Math: We model the probability of observing a pixel color $\mathbf{x}$ as a weighted sum of $K$ Gaussians:$$\mathfrak{p}(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \Sigma_k)$$$\pi_k$ (Mixing Coefficient): The "weight" or importance of the $k$-th Gaussian (e.g., "30 percent of the image belongs to the grass cluster"). $\sum \pi_k = 1$.$\boldsymbol{\mu}_k$: The center color of cluster $k$.$\Sigma_k$: The spread of cluster $k$.The Feature Vector:For every cluster $k$, we store its Weight ($\pi$), Mean ($\mu$), and Covariance ($\Sigma$).Simplification: To save space, we often assume $\Sigma_k$ is diagonal (ignoring correlations within the cluster) because the multiple clusters already handle the complex shape4.Dimension (assuming diagonal $\Sigma$):Per Gaussian: $1 (\pi) + 3 (\mu) + 3 (\text{diag}(\Sigma)) = 7$ parameters.Total: $7 \times K$ parameters.

\section{Texture}
Texture : 

\section{Image Filtering and Convolution}
\begin{itemize}
    \item Convolution and Image filtering are used to generate the words for bag of visual words. Convolution = Mathematical Engine of Computer Vision.\\
    \item Convolution takes tw functions to produce output, that corresponds to the amount of overlap between the two functions.\\
    \item image = first function, filter/kernel = second function.
    \item $f(x, y)$: The Input Image (intensity at $x, y$).
    \item $k(u, v)$: The Kernel (or Filter/Mask) of size $(2h+1) \times (2w+1)$.
    \item $g(x, y)$: The Output Image (Filtered).
\end{itemize}
$$g(x,y) = (f * k)(x,y) = \sum_{v=-h}^{h} \sum_{u=-w}^{w} k(u,v) f(x-u, y-v)$$
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Media/i10014.png}
    \label{fig:Convolution}
\end{figure}
\subsection{How Convolution Works?}
Here is the illustration of convolution example:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Media/i10015.png}
    \label{fig:Conv1}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Media/i10016.png}
    \label{fig:Conv2}
\end{figure}

\subsection{Some Mathematical results}
Convolving an image of size $H \times W$ with a filter (kernel) of size $h \times w$ (assuming $h, w$ are odd numbers like 3, 5, 7)
\textbf{Output Dimensions} are given by:
\begin{itemize}
    \item   Output Height: $H_{out} = H - h + 1$
    \item   Output Width: $W_{out} = W - w + 1$
\end{itemize}
The above results are for No Padding and Stride of 1.\\
\textbf{Padding} is adding extra border pixels around the image.\\
\textbf{Stride} is the step size with which we slide the filter over the image.\\
With Padding $P$ and Stride $S$, the output dimensions become:
\begin{itemize}
    \item   Output Height: $H_{out} = \frac{H - h + 2P}{S} + 1$
    \item   Output Width: $W_{out} = \frac{W - w + 2P}{S} + 1$
    \item  Note: Ensure that $(H - h + 2P)$ and $(W - w + 2P)$ are divisible by $S$ for integer output dimensions.
    \item  Example: For $H=32, W=32, h=5, w=5, P=2, S=1$:
    \item   $H_{out} = \frac{32 - 5 + 2*2}{1} + 1 = 32$
    \item   $W_{out} = \frac{32 - 5 + 2*2}{1} + 1 = 32$
    \item  So, the output image remains $32 \times 32$.
    \item   This is called "same" convolution because the output size is the same as the input size.
\end{itemize}

\section{Edge Detection using Convolution}
\subsection{First order edge Detection}
First order detectors use the first derivatives, which measures the rate of change of intensity. Mathematically, the gradient of the image intensity function $I(x, y)$ is given by:
$$\nabla F = \left[ \frac{\partial F}{\partial x}, \frac{\partial F}{\partial y} \right]$$
where $\frac{\partial F}{\partial x}$ are given by the formula:
$$\frac{\partial F}{\partial x} =  {F(x + 1, y) - F(x, y)}$$
and $\frac{\partial F}{\partial y}$ is given by the formula:
$$\frac{\partial F}{\partial y} =  {F(x, y + 1) - F(x, y)}$$
The magnitude of the gradient vector gives the strength of the edge:
$$|\nabla F| = \sqrt{\left( \frac{\partial F}{\partial x} \right)^2 + \left( \frac{\partial F}{\partial y} \right)^2}$$
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Media/i10017.png}
    \label{fig:Conv1}
\end{figure}

From the given figure it is evident that the direction of edge is perpendicular to the gradient direction. The direction $\theta$ of the edge can be calculated as:
$$\theta = \tan^{-1}\left( \frac{\partial F / \partial y}{\partial F / \partial x} \right)$$

After calculateting the gradient magnitude and direction, we can apply Non-maximum Suppression and Thresholding to get thin and clean edges.
\subsection{NMS - Non-Maximum Suppression} Basically, it is a technique used to thin out the edges detected in an image by retaining only the local maxima in the gradient direction.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{Media/i10018.png}
    \label{fig:NMS}
\end{figure}

\subsection{Histogram of Gradient (HOG-descriptor)}
HoG is a feature descriptor that maps the weighted distribution of gradient values to the gradient oreintaion.\\
\begin{itemize}
    \item Divide the image into small regions called cells (e.g., 8x8 pixels).
    \item For each cell, compute a histogram of gradient directions (e.g., 9 bins covering 0-180 degrees).
    \item Each pixel in the cell votes for a bin based on its gradient magnitude and orientation.
    \item Group adjacent cells into larger blocks (e.g., 2x2 cells).
    \item Normalize the histograms within each block to account for illumination variations.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Media/i12011.png}
    \caption{Histogram of Oriented Gradients (HOG) Descriptor}
    \label{fig:HOG}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Media/i12012.png}
    \caption{HOG Feature Visualization}
    \label{fig:HOG 2}
\end{figure}

Since HOG is obtained Patchwise we get a histogram for each of the patch and then we can plot the histograms as shown in the figure below:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Media/i12013.png}
    \caption{HOG Feature Visualization Patchwise}
    \label{fig:HOG 3}
\end{figure}

\section{Points of Interest (Keypoints/Interest Points)}
Mathematically, it is a point where the local image structure contains rich information. If you cut out a small patch around an interest point, that patch should look unique enough that you can find its exact twin in another image, even if the lighting or viewpoint has changed.

\paragraph{Three Catagories of points} are there in an image:
\begin{itemize}
    \item \textbf{Flat regions} are bad as once cut from the image there is no way to identify exact location.
    \item \textbf{Edge points} are better than flat regions but still not good enough as they can be identified only along one direction.
    \item \textbf{Corner points} are the best as they have high variation in all directions and therefore can be identified uniquely.
\end{itemize} 
Examples of untility of Keypoints mentioned in the class are Oreientation detection and Paranoma Stiching. In both the cases we tend to track the keypoints in both the images and depending on the use either they are matched with each other or maped across each of the images.

\section{Bag of Visual Words BoVW}
"The" Image Encoding  Technique\\
\begin{itemize}
    \item Detect Keypoints.
    \item Take a patch  sourrounding each keypoint.
    \item Describe them using HOG 
    \item Now Use BoVW to encode the image.
\end{itemize}
Before we can describe an image, we need a common vocabulary. We build this by examining the distribution of features across the entire training dataset.1. Input Data PreparationLet $\mathcal{I}_{train} = \{I_1, I_2, \dots, I_T\}$ be the set of $T$ training images.From each image $I_t$, we extract a set of local HoG descriptors.Let $\mathcal{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_M\}$ be the aggregate collection of all descriptors from all training images, where $M$ is very large (millions).Each descriptor $\mathbf{x}_i \in \mathbb{R}^d$ is a $d$-dimensional vector (e.g., for standard HoG blocks, $d$ might be 36 or 128).2. The K-Means Clustering AlgorithmWe use K-Means to group these millions of descriptors into $K$ clusters. Each cluster center represents a "Visual Word."Goal: Find a set of $K$ centroids $\mathcal{C} = \{\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_K\}$ that minimizes the Within-Cluster Sum of Squares (WCSS).Objective Function:$$J = \sum_{k=1}^{K} \sum_{\mathbf{x}_i \in S_k} ||\mathbf{x}_i - \boldsymbol{\mu}_k||^2_2$$Where $S_k$ is the set of descriptors assigned to cluster $k$.3. The Iterative OptimizationStep A (Assignment): Assign every descriptor $\mathbf{x}_i$ to the nearest centroid.$$z_i^{(t)} = \arg\min_{k} ||\mathbf{x}_i - \boldsymbol{\mu}_k^{(t)}||^2_2$$Step B (Update): Recalculate the centroids based on the new assignments.$$\boldsymbol{\mu}_k^{(t+1)} = \frac{1}{|S_k|} \sum_{\mathbf{x}_i \in S_k} \mathbf{x}_i$$Convergence: Repeat until assignments $z_i$ do not change.\\

Encoding \& Pooling (Constructing the Histogram)Now, for a specific image (training or test), we want to convert its collection of descriptors into a single fixed-length vector.1. InputAn image $I$ containing $N$ local descriptors: $X_I = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\}$.Note: $N$ varies from image to image, but we need a fixed output size.2. Vector Quantization (Hard Assignment)We map each continuous descriptor $\mathbf{x}_i$ to the index of the nearest visual word in our codebook $\mathcal{V}$. This is mathematically defined as a mapping function $q(\mathbf{x})$:$$q(\mathbf{x}_i) = \arg\min_{k \in \{1, \dots, K\}} ||\mathbf{x}_i - \boldsymbol{\mu}_k||_2$$This step discretizes the continuous feature space. A complex 128-D vector is replaced by a single integer $k$2222.3. Pooling (Histogram Construction)We aggregate these assignments into a histogram $\mathbf{h} \in \mathbb{R}^K$. This removes spatial information (we know what appeared, but not where).For each visual word $k$ (from 1 to $K$), the histogram bin $h_k$ is calculated as:$$h_k = \sum_{i=1}^{N} \delta(q(\mathbf{x}_i) - k)$$Where $\delta(\cdot)$ is the Dirac delta function (returns 1 if the argument is 0, else 0).In simple terms: Count how many descriptors in the image were assigned to Word $k$.4. NormalizationSince images have different numbers of features $N$ (e.g., a high-res image has more patches than a low-res one), raw counts are not comparable. We must normalize the histogram to transform it into a probability distribution or unit vector.L1 Normalization (Frequency): Sum of elements becomes 1.$$\mathbf{h}_{L1} = \frac{\mathbf{h}}{\sum_{j=1}^{K} h_j} = \frac{\mathbf{h}}{N}$$Interpretation: "Visual Word $k$ makes up 10percentage of the image features."L2 Normalization (Euclidean): Unit length vector (preferred for SVMs).$$\mathbf{h}_{L2} = \frac{\mathbf{h}}{||\mathbf{h}||_2} = \frac{\mathbf{h}}{\sqrt{\sum_{j=1}^{K} h_j^2}}$$

\subsection{Example: Constructing Feature Vectors (Mini-SIFT)}
To understand the contents of a feature vector $\mathbf{x}_i$ and the dataset $\mathcal{X}$, consider a simplified "Mini-SIFT" descriptor with the following parameters:
\begin{itemize}
    \item \textbf{Patch Size:} $4 \times 4$ pixels.
    \item \textbf{Grid:} $2 \times 2$ sub-regions (Quadrants: TL, TR, BL, BR).
    \item \textbf{Bins:} 2 orientation bins (Horizontal vs. Vertical).
    \item \textbf{Total Dimension:} $4 \text{ quadrants} \times 2 \text{ bins} = 8$.
\end{itemize}

\subsubsection{Step 1: Constructing Vector $\mathbf{x}_1$ (Vertical Edge)}
Consider a patch $P_1$ containing a vertical edge:
\begin{equation}
    P_1 = 
    \begin{bmatrix} 
    10 & 10 & 50 & 50 \\
    10 & 10 & 50 & 50 \\
    10 & 10 & 50 & 50 \\
    10 & 10 & 50 & 50 
    \end{bmatrix}
\end{equation}

The gradients show strong horizontal change ($G_x \approx 40$) in the right half, and zero vertical change ($G_y = 0$).

\textbf{Voting by Quadrant:}
\begin{itemize}
    \item \textbf{Top-Left (TL):} No gradients $\rightarrow [0, 0]$
    \item \textbf{Top-Right (TR):} Strong Horizontal $\rightarrow [80, 0]$
    \item \textbf{Bottom-Left (BL):} No gradients $\rightarrow [0, 0]$
    \item \textbf{Bottom-Right (BR):} Strong Horizontal $\rightarrow [80, 0]$
\end{itemize}

Concatenating these gives the feature vector $\mathbf{x}_1$:
\begin{equation}
    \mathbf{x}_1 = [ 
    \underbrace{0, 0}_{\text{TL}}, 
    \underbrace{80, 0}_{\text{TR}}, 
    \underbrace{0, 0}_{\text{BL}}, 
    \underbrace{80, 0}_{\text{BR}} 
    ]
\end{equation}

\subsubsection{Step 2: Constructing Vector $\mathbf{x}_2$ (Horizontal Edge)}
Consider a second patch $P_2$ containing a horizontal edge:
\begin{equation}
    P_2 = 
    \begin{bmatrix} 
    10 & 10 & 10 & 10 \\
    10 & 10 & 10 & 10 \\
    50 & 50 & 50 & 50 \\
    50 & 50 & 50 & 50 
    \end{bmatrix}
\end{equation}

Here, gradients are purely vertical ($G_y \approx 40$).
\begin{equation}
    \mathbf{x}_2 = [ 
    \underbrace{0, 0}_{\text{TL}}, 
    \underbrace{0, 0}_{\text{TR}}, 
    \underbrace{0, 80}_{\text{BL}}, 
    \underbrace{0, 80}_{\text{BR}} 
    ]
\end{equation}

\subsubsection{Step 3: The Data Matrix $\mathcal{X}$}
The collection $\mathcal{X}$ is the stack of all local descriptors extracted from the training images. If our dataset only contained these two patches, $\mathcal{X}$ would be:

\begin{equation}
    \mathcal{X} = 
    \begin{bmatrix} 
    \mathbf{x}_1 \\
    \mathbf{x}_2
    \end{bmatrix} = 
    \begin{bmatrix} 
    0 & 0 & 80 & 0 & 0 & 0 & 80 & 0 \\
    0 & 0 & 0 & 0 & 0 & 80 & 0 & 80
    \end{bmatrix}
\end{equation}

This matrix $\mathcal{X}$ (of size $M \times D$) is the input to the \textbf{K-Means} algorithm used to learn the Visual Dictionary.

\subsection{Example: Encoding and Pooling}

\subsubsection{Step 4: The Visual Dictionary (Codebook)}
After running K-Means on the dataset $\mathcal{X}$, assume we find $K=2$ cluster centers (Visual Words) representing the canonical features:
\begin{itemize}
    \item \textbf{Word 1 ($\boldsymbol{\mu}_1$):} Vertical Edge Prototype.
    \item \textbf{Word 2 ($\boldsymbol{\mu}_2$):} Horizontal Edge Prototype.
\end{itemize}

\subsubsection{Step 5: Encoding a New Image}
Consider a new query image from which we extract 3 patches. We compute the Euclidean distance from each patch descriptor to the dictionary centroids and assign it to the nearest word (Vector Quantization).

\begin{enumerate}
    \item \textbf{Patch A:} Descriptor matches $\boldsymbol{\mu}_1$ perfectly. \\
    $\rightarrow$ Assigned to \textbf{Word 1}.
    
    \item \textbf{Patch B:} Descriptor matches $\boldsymbol{\mu}_1$ perfectly. \\
    $\rightarrow$ Assigned to \textbf{Word 1}.
    
    \item \textbf{Patch C:} Descriptor is close to $\boldsymbol{\mu}_2$ (Horizontal). \\
    $\rightarrow$ Assigned to \textbf{Word 2}.
\end{enumerate}

\subsubsection{Step 6: Pooling (Histogram Construction)}
We aggregate the assignments into a histogram $\mathbf{h} \in \mathbb{R}^K$.

\begin{itemize}
    \item Count for Word 1: 2
    \item Count for Word 2: 1
\end{itemize}

The raw histogram is $\mathbf{h}_{raw} = [2, 1]$. To make this invariant to the number of patches (image size), we apply L1 normalization (dividing by $N=3$):

\begin{equation}
    \mathbf{h}_{final} = \left[ \frac{2}{3}, \frac{1}{3} \right] \approx [0.67, 0.33]
\end{equation}

This final vector $[0.67, 0.33]$ is the \textbf{Bag of Visual Words} representation of the new image, which serves as the input to the final classifier (e.g., SVM).

\end{document}